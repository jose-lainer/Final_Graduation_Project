{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Word2Vec_Skipgram.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hdn1RQk0ea8S"
      },
      "source": [
        "# **Word2Vec - Skipgram**\n",
        "\n",
        "## enwiki cleaned and tokenized with Gensim\n",
        "\n",
        "**API:** https://radimrehurek.com/gensim/apiref.html\n",
        "\n",
        "**Dataset:** https://dumps.wikimedia.org/enwiki/\n",
        "\n",
        "**Info:**\n",
        "\n",
        "https://towardsdatascience.com/word-embedding-with-word2vec-and-fasttext-a209c1d3e12c\n",
        "\n",
        "https://www.analyticsvidhya.com/blog/2017/06/word-embeddings-count-word2veec/\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cGcLocSDdhpA"
      },
      "source": [
        "## **1. Install packages**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1nMueiVOeYTm"
      },
      "source": [
        "#from google.colab import drive\n",
        "#drive.mount('/content/drive', force_remount=True)\n",
        "#%cd 'drive/My Drive/TFG/Code/Pre_Textual'\n",
        "import gensim\n",
        "import multiprocessing\n",
        "from gensim.corpora.wikicorpus import WikiCorpus\n",
        "from gensim.models.word2vec import Word2Vec"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ydIukeHQdpDW"
      },
      "source": [
        "## **2. Construct corpus**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YzX4jIMthpti"
      },
      "source": [
        "# To get the state of the functions\n",
        "import logging\n",
        "logging.basicConfig(format='%(asctime)s: %(levelname)s: %(message)s')\n",
        "logging.root.setLevel(level=logging.INFO)\n",
        "\n",
        "# Construct a corpus\n",
        "wiki = WikiCorpus('enwiki-20210520-pages-articles.xml.bz2')\n",
        "\n",
        "# Saves corpus in-memory state\n",
        "wiki.save('enwiki.corpus')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EfSE4VL-dySL"
      },
      "source": [
        "## **3. Yield list of tokens**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "566cTkPy4xH2"
      },
      "source": [
        "import numpy as np\n",
        "enwiki = np.load('enwiki.corpus', allow_pickle=True)\n",
        "# Iterate over the dump, yielding a list of tokens\n",
        "class MySentences(object):\n",
        "    def __iter__(self):\n",
        "        for text in enwiki.get_texts():\n",
        "            yield [word for word in text]\n",
        "            \n",
        "sentences = MySentences()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BVIVYHafd67J"
      },
      "source": [
        "## **4. Train skip-gram model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UUcv0c_xEFEi"
      },
      "source": [
        "# Word2Vec skip-gram model\n",
        "w2v = Word2Vec(sentences, \n",
        "                vector_size=300, \n",
        "                window=5, \n",
        "                min_count=5, \n",
        "                negative=5,\n",
        "                workers=multiprocessing.cpu_count(), \n",
        "                sg=1, \n",
        "                sample=1e-5,\n",
        "                epochs=10)\n",
        "\n",
        "w2v.save('w2v.model')\n",
        "\n",
        "# Save words vectors\n",
        "np.save('w2v_vectors.npy', w2v.wv.vectors)\n",
        "\n",
        "# Save words keys\n",
        "# words = np.array(list(w2v.wv.vocab.keys()))\n",
        "words = np.array(list(w2v.wv.index_to_key))\n",
        "np.save('w2v_keys.npy', words)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SWdeeQOxeKW3"
      },
      "source": [
        "## **5. Test Model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oFWHzq3y5tC6"
      },
      "source": [
        "import gensim \n",
        "model = gensim.models.Word2Vec.load('w2v.model')\n",
        "\n",
        "# Test the model\n",
        "model.wv.most_similar('computer')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w4ZjTmBbSw3b"
      },
      "source": [
        "# Main params of the model\n",
        "print(model)\n",
        "# Testing word pairs\n",
        "print(model.wv.similarity('apple', 'banana'))\n",
        "print(model.wv.similarity('car', 'bus'))\n",
        "print(model.wv.similarity('car', 'ship'))\n",
        "\n",
        "print(model.wv.similarity('apple', 'bus'))\n",
        "print(model.wv.similarity('apple', 'car'))\n",
        "\n",
        "\n",
        "print(model.wv.most_similar(positive=['car', 'minivan'], topn=5))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9rNFIfUg2IHV"
      },
      "source": [
        "## **6. Train Model with VisualGenome sentences**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8jAtAhY_1Hhj"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "# Load VisualGenome sentences\n",
        "VG_ARRAY = np.load('VG_sentences.npy', allow_pickle=True)\n",
        "VG_SENTENCES = VG_ARRAY.tolist() # Convert to list\n",
        "COUNT_SENTENCES = len(VG_SENTENCES) # Count of sentences"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kn2cFhVt1P8o"
      },
      "source": [
        "import logging\n",
        "logging.basicConfig(format='%(asctime)s: %(levelname)s: %(message)s')\n",
        "logging.root.setLevel(level=logging.INFO)\n",
        "\n",
        "import gensim \n",
        "model = gensim.models.Word2Vec.load('w2v.model')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IpC8KHBY1oj0"
      },
      "source": [
        "# Update weights with VisualGenome sentences\n",
        "model.train(corpus_iterable=VG_SENTENCES, total_examples=COUNT_SENTENCES, epochs=10)\n",
        "\n",
        "model.save('VG_w2v.model')\n",
        "\n",
        "# Save words vectors for VisualGenome\n",
        "np.save('VG_w2v_vectors.npy', model.wv.vectors)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}